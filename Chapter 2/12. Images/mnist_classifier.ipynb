{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier\n",
    "\n",
    "In this notebook you will create both, an mnist tabular dataset and a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- import the Operating System (os) module in python and any other library you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- As you can see each class has its own folder (Do it only for train). \n",
    "\n",
    "    - Iterate folder by folder ( os.listdir() )\n",
    "    - Inside each folder: \n",
    "        1.- Read the image\n",
    "        2.- Reshape it into a flat array (784,)\n",
    "        3.- Save the data into a pandas dataframe apending the column name as the class\n",
    "    - Save the data into a CSV\n",
    "\n",
    "    Note: if it takes to long try doing only 100 images per folder for the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\harsh\\\\Documents\\\\GitHub\\\\AI-Exercises\\\\Chapter 2\\\\12. Images'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 785)\n",
      "(8816, 785)\n",
      "(12993, 785)\n",
      "(17344, 785)\n",
      "(21416, 785)\n",
      "(25211, 785)\n",
      "(29348, 785)\n",
      "(33749, 785)\n",
      "(37812, 785)\n",
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "#Directory path\n",
    "path = 'dataset/trainingSet'\n",
    "dir_list = os.listdir(path)\n",
    "# Creating empty dataframe\n",
    "df1 = pd.DataFrame()\n",
    "# Creating for loop to extract all images from each folder\n",
    "for file in dir_list:\n",
    "    #Iterate folder by folder\n",
    "    imgs = os.listdir(path+'/'+file)\n",
    "    # Creating an array with all zeros\n",
    "    arr = np.zeros((len(imgs),785))\n",
    "    for i,img in enumerate(imgs):\n",
    "        #Opening the images\n",
    "        imag = Image.open(path+'/'+file+'/'+img)\n",
    "        #Converting images into array\n",
    "        arry = np.array(imag,dtype=float)\n",
    "        # Reshaping image to 1D\n",
    "        arry = arry.flatten()\n",
    "        #Adding features\n",
    "        arr[i,:784]=arry\n",
    "        #Adding target labels\n",
    "        arr[i,784]=int(file)\n",
    "    # Converting array into dataframe\n",
    "    df2 = pd.DataFrame(arr)\n",
    "    #\n",
    "    df = pd.concat([df1,df2])\n",
    "    df1 = df\n",
    "    print(df.shape)\n",
    "df.to_csv('numerical1.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/trainingSample'\n",
    "\n",
    "img = Image.open('dataset/trainingSample/0/img_1.jpg')\n",
    "arry = np.array(img, dtype=float)\n",
    "arry = arry.flatten()\n",
    "#df = pd.DataFrame(arry)\n",
    "#df\n",
    "#dir_list = os.listdir(path)\n",
    "# df = pd.DataFrame()\n",
    "# for file in dir_list:\n",
    "#     imgs = os.listdir(path+'/'+file)\n",
    "#     for img in imgs:\n",
    "#         imag = Image.open(path+'/'+file+'/'+img)\n",
    "#         arry = np.array(imag, dtype=float)\n",
    "#         #print(arry.shape)\n",
    "#         arry = arry.reshape(784,)\n",
    "#         #print(arry)\n",
    "#     df1 = pd.DataFrame({'Image': arry}).T\n",
    "#     df1['class'] = file\n",
    "#     df = df.append(df1)\n",
    "#     print(df.shape)\n",
    "#df.to_csv('numerical.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Load the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3.0  0.0  0.0.1  3.0.1  7.0  3.0.2  0.0.2  3.0.3  0.0.3  11.0  ...  \\\n",
      "0  0.0  0.0    0.0    0.0  0.0    0.0    0.0    0.0    8.0   0.0  ...   \n",
      "1  0.0  0.0    0.0    0.0  0.0    0.0    0.0    0.0    2.0   0.0  ...   \n",
      "2  0.0  0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0   9.0  ...   \n",
      "3  0.0  0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0   2.0  ...   \n",
      "4  0.0  0.0    0.0    0.0  0.0    0.0    0.0    0.0    0.0   4.0  ...   \n",
      "\n",
      "   0.0.386  0.0.387  0.0.388  0.0.389  0.0.390  0.0.391  0.0.392  0.0.393  \\\n",
      "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "   0.0.394  0.0.395  \n",
      "0      0.0      0.0  \n",
      "1      0.0      0.0  \n",
      "2      0.0      0.0  \n",
      "3      0.0      0.0  \n",
      "4      0.0      0.0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41999, 784)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('numerical1.csv')\n",
    "print(data.head())\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Create a dictionary of models (No preprocessing needed, it has already been done).\n",
    "    \n",
    "    Include both, tree models and mult models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100),\n",
    "    \"Gradientboosting\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"XGB\": XGBClassifier(n_estimators=100),\n",
    "    \"LGB\" : LGBMClassifier(n_estimators=100),\n",
    "    \"Catboost\": CatBoostClassifier(n_estimators=100)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Using either cross validation or stratification find out which is the best model\n",
    "    - Base your code on the previous two days examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingDecisionTreeModel\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, metrics\n",
    "import time\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "results =  []\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    print(\"Training\" + name + \"Model\")\n",
    "    start_time = time.time()\n",
    "    prediction = model_selection.cross_val_predict(model, X, y, cv=skf)\n",
    "    total_time = time.time() - start_time\n",
    "    print(\"Done\" + name + \"Model\")\n",
    "\n",
    "    results.append({\n",
    "        \"Model Name\": name,\n",
    "        \"Accuracy\": metrics.accuracy_score(y, prediction)*100,\n",
    "        \"Bal Acc\": metrics.balanced_accuracy_score(y, prediction),\n",
    "        \"Time\": total_time\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Can you rotate an image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4732be584c8675bd1318e2b536250c893a31e770e1f629f67b4b8f5dd5545fd2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('machine_learning': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
